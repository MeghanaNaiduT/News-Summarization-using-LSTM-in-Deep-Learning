
# ğŸ§  News Summarization using LSTM (Deep Learning Approach)

This project focuses on **extractive and abstractive text summarization** of news articles using deep learning techniques, specifically an **LSTM-based Sequence-to-Sequence (Seq2Seq) model**. The objective is to generate concise summaries from long-form news content while preserving the essential information.


## ğŸ“Œ Features

- Preprocesses and summarizes real-world news datasets
- Converts text into numerical vectors using word embeddings
- Implements a Seq2Seq LSTM model (with optional attention mechanism)
- Performs both extractive and abstractive summarization
- Evaluates the summaries using **BLEU** and **ROUGE** scores


## ğŸ—ƒï¸ Dataset

Two datasets are used:

- `news_summary.csv` â€” Smaller dataset (~4,500 entries)
- `news_summary_more.csv` â€” Larger dataset (~10,000 entries)

Each dataset contains:

- `headlines`: Title of the news article
- `text`: Main body of the news
- Optional: `author`, `date`


## ğŸ”§ Installation & Requirements

Install the required Python libraries:

```bash
pip install numpy pandas tensorflow keras scikit-learn matplotlib seaborn
```

If using **Google Colab**, mount your Google Drive as shown:

```python
from google.colab import drive
drive.mount('/content/drive')
```


## ğŸš€ How to Run

Follow these steps to run the project:

1. **Clone this repository** or upload the notebook to [Google Colab](https://colab.research.google.com/).
2. **Mount Google Drive** to access the dataset files:

   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

3. **Run the notebook cells sequentially**, in this order:

   - âœ… Check system and CPU/GPU info
   - ğŸ“Š Load and inspect the dataset
   - ğŸ§¹ Preprocess text (cleaning, tokenization, stemming)
   - ğŸ”  Apply word embedding using `Tokenizer` and padding
   - ğŸ§  Build the LSTM Seq2Seq model
   - ğŸ“ˆ Train and evaluate the model
   - ğŸ“ View the generated summaries and performance metrics


## ğŸ§  Model Architecture

The summarization model uses an **LSTM-based Sequence-to-Sequence architecture**:

- **Encoder**:
  - Embedding Layer
  - LSTM Layer
- **Decoder**:
  - Embedding Layer
  - LSTM Layer
  - Dense Output Layer
- ğŸ” *Optional*: Add an Attention Mechanism for better context
- ğŸ§‘â€ğŸ« Training Strategy: *Teacher Forcing* on padded sequences


## ğŸ“ˆ Results

- Trained for multiple epochs for better accuracy
- Summaries were more coherent and context-aware with larger datasets
- Evaluation metrics:
  - **BLEU Score**
  - **ROUGE Score**
- Experiments conducted:
  - With 1,000 rows only
  - With 2 LSTM layers
  - With the entire dataset using a single LSTM

**Example Output**:

> **Original**: The government has launched a new initiative to improve rural healthcare...  
>  
> **Generated Summary**: Government launches rural healthcare initiative.


## âš ï¸ Limitations & Future Work

- Training on larger datasets (e.g., **CNN/DailyMail**) could improve generalization
- Replacing LSTM with **transformer-based models** (e.g., BERT, T5) may improve performance
- Explore **API integration** or **web-based UI** for user-friendly deployment


## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).


## âœï¸ Credits

- **Notebook Author**: *Meghana Naidu T*
- **Guided By**: *Dr. Rajeev Sharma*
- **Dataset Source**: Public repositories on *GitHub*
